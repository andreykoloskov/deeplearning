{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import numpy.random as rnd\n",
    "\n",
    "import torch\n",
    "import torch.autograd as autograd\n",
    "from torch.autograd import Variable\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch.utils.data\n",
    "\n",
    "import time\n",
    "import os\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import shuffle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Все места, где нужно дописать код отмечены TODO."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Считывание и подготовка данных."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Считываем данные: каждый класс лежит в своем csv файле. \n",
    "male = pd.read_csv('male.csv',header = None)[0]\n",
    "female = pd.read_csv('female.csv',header = None)[0]\n",
    "\n",
    "y = np.hstack((np.zeros(len(male)),np.ones(len(female))))\n",
    "data = list(male)\n",
    "data.extend(list(female))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Для дальнейшей работы нам понадобится словарь символов + \n",
    "# мы не будем различать строчные и прописные буквы + \n",
    "# у нас все последовательности разной длины и нам нужно понимать, какова макимальная длина + \n",
    "# нам нужен отдельный символ под паддинг, чтобы уметь работать с последовательностями разной длины\n",
    "MAX_LEN = 0\n",
    "chars = set()\n",
    "for i in xrange(len(data)):\n",
    "    data[i] = data[i].lower()\n",
    "    MAX_LEN = max(MAX_LEN,len(data[i]))\n",
    "    chars = chars.union(set(data[i][:]))\n",
    "    \n",
    "chars = list(chars)\n",
    "PAD_CHAR = '_PADDING_'\n",
    "chars = [PAD_CHAR] + chars\n",
    "char_to_id = { ch:id for id,ch in enumerate(chars) }\n",
    "id_to_char = { id:ch for id,ch in enumerate(chars) }\n",
    "\n",
    "VOCAB_SIZE = len(chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Разделим выборку на трейн и тест\n",
    "X_train, X_test, y_train, y_test = train_test_split(data, y, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def data2format(data, labels):\n",
    "    \"\"\"Функция преобразует выбоку данных в формат, подходящий для подачи в нейронную сеть.\n",
    "    \n",
    "    data - список строк (пример - X_train)\n",
    "    labels - вектор меток для строк из data (пример - y_train)\n",
    "    \n",
    "    Дальше за N обозначается число строк в data\n",
    "    \n",
    "    Вернуть нужно словарь со следующими элементами:\n",
    "    x - матрица размера [N, MAX_LEN], в которой каждая строка соответствует строке в data:\n",
    "        вся строка кодируется с помощью char_to_id, недостающие элементы в конце коротких строк заполняются символом PAD_CHAR\n",
    "    y - вектор длины N с метками\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    # TODO\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_data = data2format(X_train,y_train)\n",
    "test_data = data2format(X_test,y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Вспомогательные функции"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Необходимые константы\n",
    "NUM_EPOCHS = 20\n",
    "BATCH_SIZE = 100\n",
    "SEQ_LEN = 20\n",
    "LEARNING_RATE = 0.01\n",
    "GRAD_CLIP = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Технические вещи\n",
    "\n",
    "# Вспомогательная функция для запаковки результата обучения \n",
    "def pack(train_err, train_acc, test_err, test_acc, network):\n",
    "    return {'train_err':train_err, \n",
    "        'train_acc':train_acc, \n",
    "        'test_err':test_err, \n",
    "        'test_acc':test_acc, \n",
    "        'network':network\n",
    "           } \n",
    "\n",
    "# устойчивая реализация кросс-энтропии\n",
    "def BinaryCrossentropy(probs, labels):\n",
    "    probs = probs[:, 0]\n",
    "    probs = torch.clamp(probs, 1e-7, 1-1e-7)\n",
    "    labels = labels.type(probs.data.type())\n",
    "    return -(labels * torch.log(probs) + (1 - labels) * (torch.log(1 - probs))).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Нейронная сеть"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В простейшем случае мы будем использовать сеть, которая считывает входную последовательность, и выдает результат только в самом конце."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Для работы с последовательностями разной длины стоит использовать эти функции.\n",
    "# Обратите внимание, что последователньости нужно отсортировать перед подачей в pack_padded_sequence.\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
    "\n",
    "class Net(nn.Module):\n",
    "    \"\"\"Класс задает простейшую рекуррентную сеть, которая принимает на вход батч размера [BATCH_SIZE, MAX_LEN] \n",
    "    и применяет к нему следующие преобразования:\n",
    "    \n",
    "    1. Embedding для перевода кодировки символов в нормальное представление: VOCAB_SIZE -> emb_size\n",
    "    2. Рекуррентный слой c n_hidden элементов на скрытом слое. Из этого слоя нам нужно только выход в последний момент времени.\n",
    "    3. Полносвязный слой для бинарной классификации с sigmoid в качестве нелинейности.\n",
    "    \n",
    "    * Обратите внимание на параметр batch_first у рекуррентного слоя.\n",
    "    \"\"\"\n",
    "    def __init__(self, embedding_dim, hidden_dim, vocab_size, batch_size):\n",
    "        # TODO\n",
    "\n",
    "    def forward(self, names, lengths):\n",
    "        # TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def train(train_data, test_data, emb_size, n_hidden, show = False):\n",
    "    \"\"\"Функция обучает нейросеть по данным train_data + контролирует процесс по качеству на test_data\n",
    "    Следует обратить внимание на следующее:\n",
    "    1. Сеть будем учить NUM_EPOCHS эпох, в каждой из столько батчей, сколько есть в train_data\n",
    "    2. Генерировать батчи можно с помощью батчгенератора pytorch. Для этого пригодятся torch.utils.data.TensorDataset\n",
    "        и torch.utils.data.DataLoader.\n",
    "    3. Для того, чтобы следить за процессом обучения будем считать средний loss и \n",
    "        среднюю точность классификации на всех батчах трейна и теста и сохранять эти данные \n",
    "        в соответствующие массивы. \n",
    "    4. Перед тем, как делать шаг по градиенту, будем ограничивать градиент по норме значением GRAD_CLIP\n",
    "    \n",
    "    \"\"\"\n",
    "    print(\"Prepare data ...\")\n",
    "    # задаем батчгенераторы\n",
    "    # TODO\n",
    "    \n",
    "    train_size = len(train_data['x'])\n",
    "    test_size = len(test_data['x'])\n",
    "    num_train_batches = train_size / BATCH_SIZE\n",
    "    num_test_batches = test_size / BATCH_SIZE\n",
    "    train_err=np.zeros(NUM_EPOCHS)\n",
    "    train_acc=np.zeros(NUM_EPOCHS)\n",
    "    test_err=np.zeros(NUM_EPOCHS)\n",
    "    test_acc=np.zeros(NUM_EPOCHS)\n",
    "\n",
    "    print(\"Building network ...\")\n",
    "    # Строим сеть и переносим ее на cuda, если нужно\n",
    "    # TODO\n",
    "    print(\"The network has {} params\".format(sum([x.data.numel() for x in net.parameters()])))\n",
    "    \n",
    "    # Задаем оптимизатор, рекомендуется использовать adam\n",
    "    # TODO\n",
    "    \n",
    "    print(\"Training ...\")\n",
    "    for epoch in xrange(NUM_EPOCHS):\n",
    "        start_time = time.time()\n",
    "        # TODO\n",
    "\n",
    "        print(\"Epoch {} \\t loss / accuracy test = {:.4f}, {:.4f} \\t train = {:.4f}, {:.4f} \\t time = {:.2f}s\".\n",
    "              format(epoch, test_err[epoch],test_acc[epoch], \n",
    "                     train_err[epoch],  train_acc[epoch],time.time() - start_time))\n",
    "             \n",
    "    return pack(train_err, train_acc, test_err, test_acc, net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Перед тем, как запускать обучение большой сети на большое число эпох, проверьте, что маленькая сеть выдает вменяемые результаты: качество больше 50%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prepare data ...\n",
      "Building network ...\n",
      "The network has 58101 params\n",
      "Training ...\n",
      "Epoch 0 \t loss / accuracy test = 0.0047, 0.7215 \t train = 0.0057, 0.6723 \t time = 0.56s\n",
      "Epoch 1 \t loss / accuracy test = 0.0042, 0.7563 \t train = 0.0045, 0.7631 \t time = 0.41s\n",
      "Epoch 2 \t loss / accuracy test = 0.0040, 0.7714 \t train = 0.0042, 0.7879 \t time = 0.43s\n",
      "Epoch 3 \t loss / accuracy test = 0.0039, 0.7701 \t train = 0.0039, 0.8079 \t time = 0.41s\n",
      "Epoch 4 \t loss / accuracy test = 0.0038, 0.7798 \t train = 0.0038, 0.8119 \t time = 0.42s\n",
      "Epoch 5 \t loss / accuracy test = 0.0038, 0.7819 \t train = 0.0036, 0.8182 \t time = 0.41s\n",
      "Epoch 6 \t loss / accuracy test = 0.0037, 0.7785 \t train = 0.0035, 0.8263 \t time = 0.41s\n",
      "Epoch 7 \t loss / accuracy test = 0.0036, 0.7836 \t train = 0.0033, 0.8354 \t time = 0.42s\n",
      "Epoch 8 \t loss / accuracy test = 0.0036, 0.7898 \t train = 0.0032, 0.8439 \t time = 0.40s\n",
      "Epoch 9 \t loss / accuracy test = 0.0036, 0.7898 \t train = 0.0031, 0.8451 \t time = 0.40s\n",
      "Epoch 10 \t loss / accuracy test = 0.0035, 0.7949 \t train = 0.0030, 0.8558 \t time = 0.41s\n",
      "Epoch 11 \t loss / accuracy test = 0.0036, 0.7940 \t train = 0.0029, 0.8638 \t time = 0.41s\n",
      "Epoch 12 \t loss / accuracy test = 0.0036, 0.7949 \t train = 0.0028, 0.8709 \t time = 0.43s\n",
      "Epoch 13 \t loss / accuracy test = 0.0037, 0.7949 \t train = 0.0026, 0.8748 \t time = 0.41s\n",
      "Epoch 14 \t loss / accuracy test = 0.0037, 0.7978 \t train = 0.0025, 0.8809 \t time = 0.42s\n",
      "Epoch 15 \t loss / accuracy test = 0.0040, 0.7907 \t train = 0.0024, 0.8838 \t time = 0.41s\n",
      "Epoch 16 \t loss / accuracy test = 0.0037, 0.7978 \t train = 0.0023, 0.8892 \t time = 0.40s\n",
      "Epoch 17 \t loss / accuracy test = 0.0038, 0.7966 \t train = 0.0022, 0.8890 \t time = 0.42s\n",
      "Epoch 18 \t loss / accuracy test = 0.0040, 0.7928 \t train = 0.0021, 0.8966 \t time = 0.40s\n",
      "Epoch 19 \t loss / accuracy test = 0.0039, 0.7898 \t train = 0.0020, 0.9049 \t time = 0.40s\n"
     ]
    }
   ],
   "source": [
    "model = train(train_data, test_data, 40, 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Посмотрим что из этого вышло"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def predict(name, model):\n",
    "    \"\"\"Функция выдает предсказание обученной модели model для имени name.\n",
    "    Предсказание - число из [0,1] - вероятность того, что имя женское\n",
    "    \"\"\"\n",
    "    # TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dataset = set(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is a new name\n",
      "It's male name\n",
      "0.0199103318155\n"
     ]
    }
   ],
   "source": [
    "name = 'Yaroslav'.lower()\n",
    "if name in dataset:\n",
    "    print 'This name is in our dataset'\n",
    "else:\n",
    "    print 'This is a new name'\n",
    "pred = predict(name, model)\n",
    "if pred>=0.5:\n",
    "    print \"It's female name\"\n",
    "else:\n",
    "    print \"It's male name\"\n",
    "print pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is a new name\n",
      "It's female name\n",
      "0.996361672878\n"
     ]
    }
   ],
   "source": [
    "name = 'Polina'.lower()\n",
    "if name in dataset:\n",
    "    print 'This name is in our dataset'\n",
    "else:\n",
    "    print 'This is a new name'\n",
    "pred = predict(name, model)\n",
    "if pred>=0.5:\n",
    "    print \"It's female name\"\n",
    "else:\n",
    "    print \"It's male name\"\n",
    "print pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Дополнительные пункты"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Обучение более сложной модели и контроль переобучения. Попробуйте подобрать хорошую модель RNN для данной задачи. Для этого проанализируйте качество работы модели в зависимости от ее размеров, попробуйте использовать многослойную сеть. Также нужно проконтролировать переобучение моделей. Для этого можно выделить тестовый кусок из текста и смотреть на то, как меняется loss на нем в процессе обучения. Если на графиках видно переобучение, то стоит добавить dropout слои в модель (обычный dropout до, между и после рекуррентных слоев). При использовании дропаута на стадии предсказания для нового объекта нужно ставить флаг deterministic=True.\n",
    "2. Другая архитектура 1. Попробуйте использовать не только состоянию скрытых переменных в последний момент времени, а усреднение/максимум значений скрытых переменных во все моменты времени. Попробуйте двунаправленную сеть при таком подходе. \n",
    "3. Другая архитектура 2. Попробуйте использовать не только состоянию скрытых переменных в последний момент времени, а сумму значений скрытых переменных во все моменты времени с коэффициентами attention. Попробуйте двунаправленную сеть при таком подходе. Attention коэффициент для определенного момента времени может представлять собой просто линейную комбинацию значений скрытых переменных в этот момент времени с обучаемыми весами.\n",
    "3. Визуализация. Попробуйте провизуализировать результаты. Например, для стандартной архитектуры можно посмотреть на изменение предсказания во времени: на каких элементах предсказание значительнее всего изменяется в сторону одного или другого класса? При использовании схемы из 2/3 пункта, можно смотреть на вклад каждого момента времени в результат. Так как после рекуррентного слоя у нас стоит просто линейный классификатор, то можно посмотреть, что выдает этот классификатор при применении к скрытым переменным в каждый момент времени. Таким образом выделяться те буквы, которые голосуют за один класс и те, которые голосуют за другой.\n",
    "4. Batchnorm и Layernorm. Запрограммируйте RNN c layer normalization из статьи [Lei Ba et al., 2016]. Поэкспериментируйте с применением обычной batch normalization и layer normalization, сравните результаты."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
